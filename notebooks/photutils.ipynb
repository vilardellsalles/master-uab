{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometry with Photutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photutils is an affiliated package of Astropy to provide tools for detecting and performing photometry of astronomical sources.\n",
    "\n",
    "It contains tools for:\n",
    "\n",
    "* **Background Estimation (photutils.background)**\n",
    "* **Source Detection (photutils.detection)**\n",
    "* Grouping Algorithms\n",
    "* **Aperture Photometry (photutils.aperture)**\n",
    "* PSF Photometry (photutils.psf)\n",
    "* PSF Matching (photutils.psf.matching)\n",
    "* Image Segmentation (photutils.segmentation)\n",
    "* Centroids (photutils.centroids)\n",
    "* Morphological Properties (photutils.morphology)\n",
    "* Geometry Functions (photutils.geometry)\n",
    "* Datasets (photutils.datasets)\n",
    "* Utility Functions (photutils.utils)\n",
    "\n",
    "The content of the current exercise has been adapted from this [Python course held at ICE](https://github.com/Python4AstronomersAndParticlePhysicists/PythonWorkshop-ICE) in 2017. \n",
    "\n",
    "As usual, the first step is to import the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os.path\n",
    "from glob import glob\n",
    "\n",
    "from astropy import wcs\n",
    "from astropy.io import fits\n",
    "\n",
    "# FITS files are rather old and do not conform to FITS standard\n",
    "warnings.filterwarnings(\"ignore\", category=wcs.FITSFixedWarning)\n",
    "\n",
    "%run plotfits.ipynb # Load plotting function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astrometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images of M67 have no astrometric calibration in header and, therefore, pixel positions cannot be directly transformed to sky coordinates. The usual way to calibrate a science images is by using [Astrometry.net](http://astrometry.net/) [(Lang et al., 2010)](https://ui.adsabs.harvard.edu/abs/2010AJ....139.1782L/abstract). However, the installation procedure in Windows is quite complex and requires a quite large amount of time. Therefore, the instructions to astrometrically calibrate the M67 images are shown here, but only the final producs provided in resources will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands to obtain the astrometric calibration of the M67 images are:\n",
    "\n",
    "```\n",
    "cd resources/red_data\n",
    "solve-field bf_m67_b_1.fit --config ../resources/astrometry/astrometry.cfg\n",
    "solve-field bf_m67_b_2.fit --config ../resources/astrometry/astrometry.cfg\n",
    "solve-field bf_m67_b_3.fit --config ../resources/astrometry/astrometry.cfg\n",
    "solve-field bf_m67_y_1.fit --config ../resources/astrometry/astrometry.cfg\n",
    "solve-field bf_m67_y_2.fit --config ../resources/astrometry/astrometry.cfg\n",
    "solve-field bf_m67_y_3.fit --config ../resources/astrometry/astrometry.cfg\n",
    "cp *.wcs ../resources/astrometry\n",
    "```\n",
    "\n",
    "The result of this execution already provides astrometrically calibrated FITS images (named `*.new`), but in order to illustrate the use of WCS functionality, the `*.wcs` files will be used. These files contain the tranformation from pixel positions to celestial coordinates in a format called [World Coordinate System](https://docs.astropy.org/en/stable/wcs/) (WCS), a standard for FITS files. The goal is to introduce this information into the clean M67 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wcs_calib in glob(os.path.join('..', 'resources', 'astrometry', '*.wcs')):\n",
    "    basename = os.path.basename(wcs_calib).replace('wcs', 'fit')\n",
    "    clean_name = os.path.join('..','resources', 'red_data', basename)\n",
    "    calibrated_name = clean_name.replace('bf_', 'bfa_')\n",
    "\n",
    "    wcs_header = fits.open(wcs_calib)[0].header\n",
    "    clean_image = fits.open(clean_name)\n",
    "    \n",
    "    clean_image[0].header.extend(wcs_header, update=True)\n",
    "    clean_image.writeto(calibrated_name, overwrite=True)\n",
    "\n",
    "plot(calibrated_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once astrometric calibration is introduced in the images, some information can be retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "wcs_info = wcs.WCS(clean_image[0].header)\n",
    "pixel_scaling = wcs.utils.proj_plane_pixel_scales(wcs_info) * u.degree\n",
    "pixel_scale = np.mean(pixel_scaling).to(u.arcsec)\n",
    "print('Pixel scale is:', pixel_scale)\n",
    "\n",
    "corners = SkyCoord(wcs_info.calc_footprint(), unit=(u.degree, u.degree))\n",
    "corners.to_string('hmsdms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photometry\n",
    "\n",
    "### Image statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before performing any photometry we need to have a first guess of the image background properties of our science images of M67. Let's get the basic statistics of it. For that we will need to remove the sources using a sigma clipping routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import sigma_clipped_stats, median_absolute_deviation\n",
    "from astropy.io import fits\n",
    "from astropy import table\n",
    "\n",
    "rows = []\n",
    "for filename in glob(os.path.join('..', 'resources', 'red_data', 'bfa_*.fit')):\n",
    "    print('Reading', filename)\n",
    "    extension = fits.open(filename)[0]\n",
    "    values = list(sigma_clipped_stats(extension.data, sigma=3.0, maxiters=5))\n",
    "    rows.append([filename] + values + [extension.data.T])\n",
    "\n",
    "columns = ['Filename', 'Mean', 'Median', 'Deviation', 'Image']\n",
    "statistics = table.Table(rows=rows, names=columns)\n",
    "statistics.sort('Filename')\n",
    "statistics.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection\n",
    "\n",
    "To detect the sources inside a astronomical image [Photutils](https://photutils.readthedocs.io/en/stable/) provides several implementations. We will use `DAOStarFinder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import DAOStarFinder\n",
    "sigma_detection = 5.0\n",
    "fwhm = 1.5 * u.arcsec / pixel_scale\n",
    "\n",
    "sources = []\n",
    "for image in statistics:\n",
    "    filename = image['Filename']\n",
    "    std = image['Deviation']\n",
    "    median = image['Median']\n",
    "    daofind = DAOStarFinder(fwhm=fwhm.value, threshold=sigma_detection*std)\n",
    "    photometry = daofind(image['Image'] - median)\n",
    "    photometry['Filename'] = [filename] * len(photometry)\n",
    "    sources.append(photometry)\n",
    "\n",
    "catalogs = table.vstack(sources).group_by('Filename')\n",
    "catalogs[:10].show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to validate the result, the data of the first image can be plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = catalogs.groups.keys[0][0]\n",
    "\n",
    "plot(filename)\n",
    "\n",
    "sources = catalogs.groups[0]\n",
    "plt.scatter(sources['xcentroid'], sources['ycentroid'], alpha=0.5, color='limegreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the objects detected are not real stars, but CCD artifacts. The best way to identify them is to evaluate their `sharpness` and `roundness`. We are going to select only the stars with `sharpness` and `roundness` around the median values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = []\n",
    "for filename, sources in zip(catalogs.groups.keys, catalogs.groups):\n",
    "    sharpness = sources['sharpness']\n",
    "    roundness = sources['roundness1']\n",
    "\n",
    "    sharp_med = np.median(sharpness)\n",
    "    sharp_mas = median_absolute_deviation(sharpness)\n",
    "    round_med = np.median(roundness)\n",
    "    round_mas = median_absolute_deviation(roundness)\n",
    "\n",
    "    mask = (sharpness - sharp_med)**2 < 9 * sharp_mas**2 \n",
    "    mask &= (roundness - round_med)**2 < 9 * round_mas**2\n",
    "    mask &= sources['peak'] < 50000\n",
    "\n",
    "    valid += list(mask)\n",
    "\n",
    "    print(f'Found {len(sources[mask])} of {len(sources)} '\n",
    "          f'valid stars in {filename[0]}')\n",
    "\n",
    "catalogs.add_column(valid, index=1, name='valid')\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.scatter(sharpness, roundness)\n",
    "plt.scatter(sharpness[mask], roundness[mask], color='red')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate what has been done, the first image can be plotted again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(filename[0])\n",
    "\n",
    "plt.scatter(sources['xcentroid'], sources['ycentroid'], alpha=0.5, color='limegreen')\n",
    "plt.scatter(sources['xcentroid'][~mask], sources['ycentroid'][~mask], alpha=0.5, color='red')\n",
    "catalogs[:10].show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, stars not fulfilling the criteria are considered invalid. \n",
    "\n",
    "An important aspect to be addressed is that the images looks properly calibrated, but the photometry table does not contain the sky positions of the stars. To solve that `wcs` can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = None\n",
    "for filename, sources in zip(catalogs.groups.keys, catalogs.groups):\n",
    "    wcs_transform = wcs.WCS(fits.open(filename[0])[0].header)\n",
    "    new_pos = SkyCoord.from_pixel(sources['ycentroid'], sources['xcentroid'],\n",
    "                                  wcs_transform)\n",
    "\n",
    "    if coords:\n",
    "        coords = coords.insert(len(coords), new_pos)\n",
    "    else:\n",
    "        coords = new_pos\n",
    "\n",
    "if 'ra' in catalogs.colnames:\n",
    "    del catalogs['ra']\n",
    "if 'dec' in catalogs.colnames:\n",
    "    del catalogs['dec']\n",
    "\n",
    "catalogs.add_column(coords.ra, index=2, name='ra')\n",
    "catalogs.add_column(coords.dec, index=3, name='dec')\n",
    "\n",
    "catalog_name = os.path.join('..', 'resources', 'red_data', \n",
    "                            'instrumental_photometry.txt')\n",
    "catalogs.write(catalog_name, format='ascii.fixed_width', overwrite=True)\n",
    "\n",
    "catalogs[:10].show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSF Modelling (Not given in the course, to be completed)\n",
    "\n",
    "We assumed that the image had a typical value of 1.5\" per pixel. But we can make a more accurate estimation by fitting the pixels to a moffat profile around a detected star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's select the source we want to use for PSF modelling. For that, we are going to select the brightest **valid** star:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_sources = catalogs[:0]\n",
    "for filename, sources in zip(catalogs.groups.keys, catalogs.groups):\n",
    "    sources.sort('mag')\n",
    "    selected = sources[sources['valid']][0]\n",
    "    psf_sources.add_row(selected)\n",
    "    \n",
    "psf_sources.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the problem, we turn the 2D profile into a 1D distance array from the center of each pixel to the centroid of the source estimated by DAOPhot. As we intend to find the profile of the source, we need to remove the possible sky background that lies behind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_counts = []\n",
    "pixel_distance = []\n",
    "\n",
    "# Median bkg subtracted image\n",
    "bkg_subtracted_image = image - median\n",
    "\n",
    "x_cen = int(isource['xcentroid'])\n",
    "y_cen = int(isource['ycentroid'])\n",
    "\n",
    "# Pixels around detection loop\n",
    "analysis_radius = 10\n",
    "for x in range(x_cen - analysis_radius, x_cen + analysis_radius):\n",
    "    for y in range(y_cen - analysis_radius, y_cen + analysis_radius):\n",
    "        flux_counts.append(((bkg_subtracted_image[y][x]) / isource['peak']))\n",
    "        pixel_distance.append(np.linalg.norm((isource['xcentroid'] - x, isource['ycentroid'] - y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present two possible models that can fit the PSF distribution. A Gaussian and a Moffat profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling import models, fitting\n",
    "\n",
    "model = 'moffat'\n",
    "\n",
    "if model == 'gaussian':\n",
    "    # Fit the data using a Gaussian\n",
    "    fwhm_best_guess = 1.5\n",
    "    model_init = models.Gaussian1D(amplitude=1.0, mean=0, stddev=fwhm_best_guess)\n",
    "elif model == 'moffat':\n",
    "    # Fit the data using a Moffat\n",
    "    model_init = models.Moffat1D(amplitude=1.0, x_0=0, gamma=2., alpha=3.5)\n",
    "else:\n",
    "    raise Exception(\"Unknown model type: %s. Must be gaussian or moffat.\" % model)\n",
    "\n",
    "fitter = fitting.SimplexLSQFitter()\n",
    "fitted_model = fitter(model_init, pixel_distance, flux_counts)\n",
    "\n",
    "print (\"Fit value:\",  fitter.fit_info['final_func_val'])\n",
    "print (\"SN:\", isource['flux'] * sigma_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fitted the models, we need to convert from the parameters to the actual FWHM estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FHWM conversion\n",
    "if model == 'gaussian':\n",
    "    iFWHM = 2.355 * fitted_model.stddev * sdss_pixelscale.value\n",
    "elif model == 'moffat':\n",
    "    iFWHM = fitted_model.gamma * 2 * np.sqrt(2 ** (1. / fitted_model.alpha) - 1) * sdss_pixelscale.value\n",
    "else:\n",
    "    raise Exception(\"Unknown model type: %s. Must be gaussian or moffat.\" % model)\n",
    "\n",
    "print (\"FWHM estimated: \", iFWHM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally plot and see how the model traces the pixel values traces our fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check fitting\n",
    "if fitter.fit_info['final_func_val'] < 5.0:\n",
    "    color = 'green'\n",
    "else:\n",
    "    color = 'red'\n",
    "    \n",
    "# Plot the data with the best-fit model\n",
    "plt.figure()\n",
    "plt.plot(pixel_distance, flux_counts, 'ko')\n",
    "rx = np.linspace(0, int(max(pixel_distance)), int(max(pixel_distance)) * 10)\n",
    "plt.plot(rx,\n",
    "         fitted_model(rx),\n",
    "         color=color,\n",
    "         lw=3.0,\n",
    "         label='SN: %.2f, Fit: %.2f, FWHM: %.2f\"' % (isource['flux'] * sigma_detection,\n",
    "                                                       fitter.fit_info['final_func_val'],\n",
    "                                                       iFWHM))\n",
    "plt.xlabel('Distance (pixels)')\n",
    "plt.ylabel('Normalized flux (ADU)')\n",
    "plt.title('%s profile fit' % model)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the case with non-uniform background, the constant median can be insuficient. Here we produce a 2D model of the background that can be subtracted from the original image to improve the modelling of the stars close to a very large extended source (or when the backrgound is not flat for any other reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import Background2D, SigmaClip, MedianBackground\n",
    "sigma_clip = SigmaClip(sigma=3., iters=10)\n",
    "bkg_estimator = MedianBackground()\n",
    "bkg = Background2D(data=sdss_g_hdu_list[0].data, \n",
    "                   box_size=(50, 50), \n",
    "                   filter_size=(3, 3),\n",
    "                   sigma_clip=sigma_clip, \n",
    "                   bkg_estimator=bkg_estimator)\n",
    "my_python_ds9(bkg.background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go back to where the background was subtracted to verify the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperture photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the simple circular apertures. First we need to set the size we want and create the aperture objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import CircularAperture\n",
    "\n",
    "aperture_radius = 5.0\n",
    "\n",
    "positions = (sources['xcentroid'], sources['ycentroid'])\n",
    "apertures = CircularAperture(positions, r=aperture_radius)\n",
    "\n",
    "my_python_ds9(sdss_g_hdu_list[0].data[0:400, 0:400])\n",
    "apertures.plot(color='limegreen', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we only want the flux from the sources we are interested in, we need to remove the contribution from the background. The simplest way is to remove the median value constant of the sigma clipped image that we calculated before to the entire array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import aperture_photometry\n",
    "\n",
    "bkg_subtracted_image = sdss_g_hdu_list[0].data - median\n",
    "phot_table_global_bkg = aperture_photometry(bkg_subtracted_image, apertures)\n",
    "print (phot_table_global_bkg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also remove the 2D background model that we calculated before, but it is usually more precise to create an annulus around the source we are interested in, and estimate the background level there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils import CircularAnnulus\n",
    "\n",
    "r_in = 10\n",
    "r_out = 15\n",
    "\n",
    "annulus_apertures = CircularAnnulus(positions, r_in=r_in, r_out=r_out)\n",
    "\n",
    "my_python_ds9(sdss_g_hdu_list[0].data[0:400, 0:400])\n",
    "apertures.plot(color='limegreen', lw=2)\n",
    "annulus_apertures.plot(color='purple', lw=2, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circular apertures and the annulus can be joined for a common photometry processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apers = [apertures, annulus_apertures]\n",
    "phot_table_local_bkg = aperture_photometry(sdss_g_hdu_list[0].data, apers)\n",
    "print(phot_table_local_bkg)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the aperture_sum_1 to estimate the level of background around the source. We need to know the area of the annulus for this estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_mean = phot_table_local_bkg['aperture_sum_1'] / annulus_apertures.area()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can remove the background estimation to all pixels in the aperture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_sum = bkg_mean * apertures.area()\n",
    "final_sum = phot_table_local_bkg['aperture_sum_0'] - bkg_sum\n",
    "phot_table_local_bkg['residual_aperture_sum'] = final_sum\n",
    "print(phot_table_local_bkg['residual_aperture_sum'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this comparison we see that many sources have the same flux with both methods but a significant amount of sources (the ones in the galaxy halo) have significantly more flux in the global subtraction method, as the flux from M102 is added to the one of the stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(phot_table_local_bkg['residual_aperture_sum'], phot_table_global_bkg['aperture_sum'])\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0,100)\n",
    "plt.xlabel('Local background')\n",
    "plt.ylabel('Global background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot to verify this! Astropy qtables work similarly to Pandas, so we can make iloc searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise PhotUtils:</span> Identify problematic sources\n",
    "Locate which sources differ their flux in more than 50% between the measurement with local and global background estimation. Plot the results on the image.\n",
    "\n",
    "TIP: Using the \"pandas-like\" tools can facilitate the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 43-52 solutions/10_Astronomy.py\n",
    "#phot_table_local_bkg.add_index('id')\n",
    "very_different = phot_table_local_bkg[phot_table_local_bkg['residual_aperture_sum'] * 1.5\n",
    "                                           < phot_table_global_bkg['aperture_sum']]\n",
    "\n",
    "quite_similar = phot_table_local_bkg[phot_table_local_bkg['residual_aperture_sum'] * 1.5\n",
    "                                           > phot_table_global_bkg['aperture_sum']]\n",
    "\n",
    "my_python_ds9(sdss_g_hdu_list[0].data)\n",
    "plt.scatter(very_different['xcenter'], very_different['ycenter'], c='red', alpha=0.5)\n",
    "plt.scatter(quite_similar['xcenter'], quite_similar['ycenter'], c='limegreen', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing to annulus pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default when calling aperture_photometry, we only receive the sum of pixels and therefore we can only access to the mean of the pixel values inside an aperture. To properly measure the local background, we would need to get the median of the pixels in the annulus and preferably, perform a sigma clipping over the values.\n",
    "\n",
    "Recently photutils enabled masks so we can actually do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat annulus - 11\n",
    "# Source in annulus - 12\n",
    "\n",
    "source_num = 12\n",
    "\n",
    "x = phot_table_local_bkg[source_num]['xcenter'].value\n",
    "y = phot_table_local_bkg[source_num]['ycenter'].value\n",
    "\n",
    "stamp_radius = 20\n",
    "my_python_ds9(sdss_g_hdu_list[0].data[int(y - stamp_radius):\n",
    "                                      int(y + stamp_radius), \n",
    "                                      int(x - stamp_radius): \n",
    "                                      int(x + stamp_radius)])\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to the annulus mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create annulus mask\n",
    "annulus_apertures = CircularAnnulus((x, y), r_in=r_in, r_out=r_out)\n",
    "masks = annulus_apertures.to_mask(method='center')\n",
    "m0 = masks[0]\n",
    "\n",
    "plt.imshow(m0)\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply it to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut to data\n",
    "cutout_data = m0.apply(sdss_g_hdu_list[0].data)\n",
    "\n",
    "plt.imshow(cutout_data)\n",
    "plt.grid('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to the pixels, we can perform the median and compare to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annulus_array = cutout_data[cutout_data != 0]\n",
    "\n",
    "# Apply statistics to masked data\n",
    "mean = np.mean(annulus_array)\n",
    "median = np.median(annulus_array)\n",
    "std = np.std(annulus_array)\n",
    "print (mean, median, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even sigma-clip the sources that may appear on top of the background. This creates a numpy.masked array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import sigma_clip\n",
    "\n",
    "clip_annulus_array = sigma_clip(annulus_array, sigma=3, iters=2)\n",
    "\n",
    "mean_clipped = np.ma.mean(clip_annulus_array)\n",
    "median_clipped = np.ma.median(clip_annulus_array)\n",
    "std_clipped = np.ma.std(clip_annulus_array)\n",
    "print (mean_clipped, median_clipped, std_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise PhotUtils 2:</span> Estimate aperture error\n",
    "Compute what would be the measurement error for the last aperture (the one with sigma clipping)\n",
    "\n",
    "$$\n",
    "\\sigma^2_{F} = \\sum_i{\\sigma^2_{i}+F/g}\n",
    "$$\n",
    "\n",
    "TIP: Gain is the exposure time when pixels are in e/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 56-61 solutions/10_Astronomy.py\n",
    "g = float(sdss_g_hdu_list[0].header['EXPTIME'])\n",
    "aperture_area = apertures.area()  # or also = math.pi * aperture_radius**2\n",
    "F = phot_table_local_bkg['aperture_sum_1'][source_num] - (median_clipped * aperture_area)\n",
    "sigma_F = np.sqrt((F / g) + (aperture_area * std_clipped ** 2))\n",
    "\n",
    "print (phot_table_local_bkg['aperture_sum_1'][source_num],  sigma_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this link you can find details on how to estimate the error:\n",
    "http://photutils.readthedocs.io/en/stable/photutils/aperture.html#error-estimation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
